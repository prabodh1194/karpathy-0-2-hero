{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4aada1eed58194",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T08:30:43.556890Z",
     "start_time": "2025-11-23T08:30:43.545251Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from nn import MLP\n",
    "from micrograd import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c02d3336ffd3342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T08:30:44.645896Z",
     "start_time": "2025-11-23T08:30:44.580627Z"
    }
   },
   "outputs": [],
   "source": [
    "n = MLP(784, [128, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l54mx3agd5j",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare the Data\n",
    "\n",
    "MNIST images are 28×28 pixels = 784 values. We need to:\n",
    "1. Load the data\n",
    "2. **Flatten** each image: (28, 28) → 784-element vector\n",
    "3. **Normalize** pixels: 0-255 → 0.0-1.0 (helps training)\n",
    "4. Convert to Value objects (for autodiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "l22suq1kmgc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T08:30:45.967002Z",
     "start_time": "2025-11-23T08:30:45.787958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 28, 28)\n",
      "Training labels: (60000,)\n",
      "Pixel range: 0 to 255\n"
     ]
    }
   ],
   "source": [
    "from mnist_loader import load_mnist\n",
    "\n",
    "# Load the data\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")  # (60000, 28, 28)\n",
    "print(f\"Training labels: {y_train.shape}\")  # (60000,)\n",
    "print(f\"Pixel range: {X_train.min()} to {X_train.max()}\")  # 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "r1p9ym9v1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T08:30:49.155619Z",
     "start_time": "2025-11-23T08:30:49.146911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened shape: (10, 784)\n",
      "Normalized range: 0.00 to 1.00\n"
     ]
    }
   ],
   "source": [
    "# Start small - use only 100 samples for fast experimentation\n",
    "n_samples = 10\n",
    "X_small = X_train[:n_samples]\n",
    "y_small = y_train[:n_samples]\n",
    "\n",
    "# Flatten: (100, 28, 28) → (100, 784)\n",
    "# reshape(-1, 784) means: keep batch size automatic, make each row 784 elements\n",
    "X_flat = X_small.reshape(-1, 784)\n",
    "\n",
    "# Normalize: 0-255 → 0.0-1.0\n",
    "# Why? Keeps gradients stable. Large numbers (255) can cause exploding gradients\n",
    "X_norm = X_flat / 255.0\n",
    "\n",
    "print(f\"Flattened shape: {X_flat.shape}\")  # (100, 784)\n",
    "print(f\"Normalized range: {X_norm.min():.2f} to {X_norm.max():.2f}\")  # 0.00 to 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mp7ee8m4sni",
   "metadata": {},
   "source": [
    "## Step 2: Define the Loss Function\n",
    "\n",
    "**What's a loss?** A number that measures how wrong your predictions are.\n",
    "- Low loss = good predictions\n",
    "- High loss = bad predictions\n",
    "\n",
    "### Loss Function Comparison\n",
    "\n",
    "Imagine you're classifying a digit as \"3\".\n",
    "\n",
    "Your network outputs 10 scores (one for each digit 0-9).\n",
    "\n",
    "Let's say the raw scores are:\n",
    "`[1.2, 0.5, 2.1, 4.5, 1.8, 0.9, 1.1, 2.0, 0.7, 1.3]`\n",
    "\n",
    "The correct answer is digit 3 (index 3, which has score 4.5 - the highest!). Now let's see how each loss function \"thinks\" about this:\n",
    "\n",
    "#### 1) **MSE Loss (Mean Squared Error)**\n",
    "MSE wants your outputs to match a target vector exactly.\n",
    "\n",
    "First, convert the label to **one-hot encoding**:\n",
    "- Label: 3 → Target: `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`\n",
    "- (Only the correct class gets 1, all others get 0)\n",
    "\n",
    "Your output: `[1.2, 0.5, 2.1, 4.5, 1.8, 0.9, 1.1, 2.0, 0.7, 1.3]`\n",
    "\n",
    "MSE calculates: `(1.2-0)² + (0.5-0)² + (2.1-0)² + (4.5-1)² + ...`\n",
    "\n",
    "The problem:\n",
    "- It's angry that digit 2 has score 2.1 instead of 0.0\n",
    "- It's angry that digit 3 has score 4.5 instead of 1.0\n",
    "- It treats these errors equally!\n",
    "\n",
    "**What MSE is saying**: \"I don't care that you picked the right digit. Your numbers are wrong!\"\n",
    "\n",
    "It's like a teacher marking your test wrong because you wrote \"3.0\" instead of exactly \"3\".\n",
    "\n",
    "#### 2) **Hinge Loss (Multi-class SVM)**\n",
    "Hinge loss asks: \"Is the correct class winning by at least a margin of 1.0 against EVERY wrong class?\"\n",
    "\n",
    "**Formula**: `L = Σ(i≠y) max(0, s_i - s_y + 1)` \n",
    "- where `y` = correct class index (3)\n",
    "- `s_y` = score of correct class (4.5)\n",
    "- `s_i` = score of each wrong class\n",
    "- We sum up penalties for ALL wrong classes that are too close\n",
    "\n",
    "**Complete Dry Run:**\n",
    "\n",
    "Scores: `[1.2, 0.5, 2.1, 4.5, 1.8, 0.9, 1.1, 2.0, 0.7, 1.3]`  \n",
    "True label: 3 (so s_y = 4.5)\n",
    "\n",
    "For each wrong class i, calculate: max(0, s_i - 4.5 + 1)\n",
    "\n",
    "- Class 0: max(0, 1.2 - 4.5 + 1) = max(0, -2.3) = **0** ✓\n",
    "- Class 1: max(0, 0.5 - 4.5 + 1) = max(0, -3.0) = **0** ✓\n",
    "- Class 2: max(0, 2.1 - 4.5 + 1) = max(0, -1.4) = **0** ✓\n",
    "- ~~Class 3~~: (skip - this is the correct class)\n",
    "- Class 4: max(0, 1.8 - 4.5 + 1) = max(0, -1.7) = **0** ✓\n",
    "- Class 5: max(0, 0.9 - 4.5 + 1) = max(0, -2.6) = **0** ✓\n",
    "- Class 6: max(0, 1.1 - 4.5 + 1) = max(0, -2.4) = **0** ✓\n",
    "- Class 7: max(0, 2.0 - 4.5 + 1) = max(0, -1.5) = **0** ✓\n",
    "- Class 8: max(0, 0.7 - 4.5 + 1) = max(0, -2.8) = **0** ✓\n",
    "- Class 9: max(0, 1.3 - 4.5 + 1) = max(0, -2.2) = **0** ✓\n",
    "\n",
    "**Total Loss = 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 = 0** ✓\n",
    "\n",
    "**What Hinge Loss is saying**: \"You got it right by a comfortable margin (at least 1.0 from every wrong class). Perfect!\"\n",
    "\n",
    "**The limitation - A Different Example:**\n",
    "\n",
    "Now imagine the scores were: `[2.0, 1.0, 2.0, 4.1, 2.0, 1.5, 1.0, 3.9, 1.0, 2.0]`  \n",
    "True label: still 3 (so s_y = 4.1)\n",
    "\n",
    "Notice: Class 7 has score 3.9, very close to the correct answer!\n",
    "\n",
    "Let's recalculate:\n",
    "\n",
    "- Class 0: max(0, 2.0 - 4.1 + 1) = max(0, -1.1) = **0** ✓\n",
    "- Class 1: max(0, 1.0 - 4.1 + 1) = max(0, -2.1) = **0** ✓\n",
    "- Class 2: max(0, 2.0 - 4.1 + 1) = max(0, -1.1) = **0** ✓\n",
    "- ~~Class 3~~: (skip - correct class)\n",
    "- Class 4: max(0, 2.0 - 4.1 + 1) = max(0, -1.1) = **0** ✓\n",
    "- Class 5: max(0, 1.5 - 4.1 + 1) = max(0, -1.6) = **0** ✓\n",
    "- Class 6: max(0, 1.0 - 4.1 + 1) = max(0, -2.1) = **0** ✓\n",
    "- Class 7: max(0, 3.9 - 4.1 + 1) = max(0, **0.8**) = **0.8** ⚠️ PENALTY!\n",
    "- Class 8: max(0, 1.0 - 4.1 + 1) = max(0, -2.1) = **0** ✓\n",
    "- Class 9: max(0, 2.0 - 4.1 + 1) = max(0, -1.1) = **0** ✓\n",
    "\n",
    "**Total Loss = 0 + 0 + 0 + 0 + 0 + 0 + 0.8 + 0 + 0 = 0.8** ⚠️\n",
    "\n",
    "**The problem**: Even though you predicted correctly (class 3 is still the highest), hinge loss penalizes you because class 7 is too close. The margin between them is only 0.2 (4.1 - 3.9), but hinge loss wants at least 1.0.\n",
    "\n",
    "This forces the network to be \"confidently correct\" - not just barely winning.\n",
    "\n",
    "#### 3) **Softmax + Cross-Entropy**\n",
    "This is a two-step process:\n",
    "\n",
    "**Step 1: Softmax** converts scores to probabilities:\n",
    "```\n",
    "Raw scores: [1.2, 0.5, 2.1, 4.5, 1.8, 0.9, 1.1, 2.0, 0.7, 1.3]\n",
    "Probabilities: [0.04, 0.02, 0.09, 0.70, 0.06, 0.03, 0.03, 0.08, 0.02, 0.04]\n",
    "```\n",
    "(They now sum to 1.0 - these are \"confidences\")\n",
    "\n",
    "**Step 2: Cross-Entropy** asks: \"What probability did you assign to the correct class?\"\n",
    "\n",
    "- You said: 0.70 (70% confident it's a 3)\n",
    "- Loss = -log(0.70) = 0.36\n",
    "\n",
    "If you had been more confident:\n",
    "- 90% confident → Loss = -log(0.90) = 0.11 (better!)\n",
    "- 99% confident → Loss = -log(0.99) = 0.01 (even better!)\n",
    "\n",
    "If you had been less confident:\n",
    "- 50% confident → Loss = -log(0.50) = 0.69 (worse!)\n",
    "- 10% confident → Loss = -log(0.10) = 2.30 (much worse!)\n",
    "- 1% confident → Loss = -log(0.01) = 4.61 (terrible!)\n",
    "\n",
    "**What Cross-Entropy is saying**: \"You got it right, but you're only 70% sure. Let's push you to be MORE confident in the correct answer.\"\n",
    "\n",
    "It's never satisfied! Even at 99.9% it's saying \"you can do better.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Comparison\n",
    "\n",
    "| Loss Function | What it optimizes | When to use | Pros | Cons |\n",
    "|--------------|-------------------|-------------|------|------|\n",
    "| **MSE** | Match exact target values | Regression, simple cases | Simple, easy to implement | Not ideal for classification |\n",
    "| **Hinge Loss** | Win by a margin | Binary/multi-class SVM | Forces confident boundaries | Doesn't care beyond margin |\n",
    "| **Cross-Entropy** | Maximize probability of correct class | Multi-class classification | Industry standard, best performance | Requires softmax, slightly complex |\n",
    "\n",
    "**For this tutorial**: We'll start with **MSE** (simplest to understand and implement). Once you see it working, you can experiment with the others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "345e3902a0a8c876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T08:32:05.339914Z",
     "start_time": "2025-11-23T08:32:03.588886Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/9g0xpf9j7_3ctlwn4fwzcwjc0000gn/T/ipykernel_79904/838481714.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"[{i} - {datetime.utcnow().isoformat()}] Time taken: {t1 - t0:.3f}s\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 2025-11-23T08:32:03.781134] Time taken: 0.176s\n",
      "[1 - 2025-11-23T08:32:03.947789] Time taken: 0.167s\n",
      "[2 - 2025-11-23T08:32:04.116396] Time taken: 0.169s\n",
      "[3 - 2025-11-23T08:32:04.295041] Time taken: 0.179s\n",
      "[4 - 2025-11-23T08:32:04.458621] Time taken: 0.164s\n",
      "[5 - 2025-11-23T08:32:04.634666] Time taken: 0.176s\n",
      "[6 - 2025-11-23T08:32:04.799985] Time taken: 0.165s\n",
      "[7 - 2025-11-23T08:32:04.979981] Time taken: 0.180s\n",
      "[8 - 2025-11-23T08:32:05.153571] Time taken: 0.174s\n",
      "[9 - 2025-11-23T08:32:05.337028] Time taken: 0.183s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Value(data=9.773719152846573), 0.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn() -> tuple[Value, float]:\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_norm]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i, x in enumerate(inputs):\n",
    "        t0 = time.time()\n",
    "        predictions.append(n(x))\n",
    "        t1 = time.time()\n",
    "        print(f\"[{i} - {datetime.utcnow().isoformat()}] Time taken: {t1 - t0:.3f}s\")\n",
    "\n",
    "    # For each sample:\n",
    "    #   - Get the true label (y_small[i])\n",
    "    #   - Create one-hot target: [0,0,0,1,0,0,0,0,0,0] if label is 3\n",
    "    #   - Calculate (pred - target)^2 for all 10 outputs\n",
    "    #   - Sum them up\n",
    "    # Then average across all samples\n",
    "\n",
    "    losses = []\n",
    "    for i in range(n_samples):\n",
    "        # Get prediction (list of 10 Values)\n",
    "        pred = predictions[i]\n",
    "\n",
    "        # Get true label\n",
    "        true_label = y_small[i]\n",
    "\n",
    "        target = [0] * 10\n",
    "        target[true_label] = 1\n",
    "\n",
    "        sample_loss = sum((pred[j] - target[j]) ** 2 for j in range(10))\n",
    "        losses.append(sample_loss)\n",
    "\n",
    "    total_loss = sum(losses) / (1.0 * n_samples)\n",
    "\n",
    "    # For each sample, check if argmax(prediction) == true_label\n",
    "    # Hint: Find which output has the highest .data value\n",
    "    correct = 0.0\n",
    "    for i in range(n_samples):\n",
    "        predicted_digit = max(range(10), key=lambda j: predictions[i][j].data)\n",
    "\n",
    "        if predicted_digit == y_small[i]:\n",
    "            correct += 1\n",
    "\n",
    "    return total_loss, correct / n_samples\n",
    "\n",
    "\n",
    "loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vhhd8ncfzyj",
   "metadata": {},
   "source": "## Step 4: Training Loop (Gradient Descent)\n\nNow we train the network! This is where the magic happens.\n\n**The Training Algorithm:**\n1. **Forward pass**: Calculate loss (measure how wrong we are)\n2. **Backward pass**: Calculate gradients (which direction to adjust weights)\n3. **Update weights**: Move in the opposite direction of gradients\n\n**Gradient Descent Intuition:**\n- Imagine you're lost in foggy mountains trying to get to the bottom (minimum loss)\n- You can't see far, but you can feel which way is downhill (gradient)\n- Take a small step downhill (learning_rate × gradient)\n- Repeat until you reach the bottom!\n\n**The gradient** tells us: \"If you increase this weight, loss will increase/decrease by this much\"\n\nWe want to **decrease** loss, so we move weights in the **opposite** direction of the gradient."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ijf968un",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T08:35:36.914651Z",
     "start_time": "2025-11-23T08:33:23.057573Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/9g0xpf9j7_3ctlwn4fwzcwjc0000gn/T/ipykernel_79904/838481714.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"[{i} - {datetime.utcnow().isoformat()}] Time taken: {t1 - t0:.3f}s\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 2025-11-23T08:33:23.262171] Time taken: 0.186s\n",
      "[1 - 2025-11-23T08:33:23.427211] Time taken: 0.165s\n",
      "[2 - 2025-11-23T08:33:23.614672] Time taken: 0.187s\n",
      "[3 - 2025-11-23T08:33:23.828028] Time taken: 0.213s\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "learning_rate = 0.01  # How big of a step to take\n",
    "n_steps = 5  # Number of training iterations\n",
    "\n",
    "for k in range(n_steps):\n",
    "    total_loss, accuracy = loss_fn()\n",
    "\n",
    "    n.zero_grad()\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    for p in n.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Print progress every 10 steps\n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k}: loss {total_loss.data:.4f}, accuracy {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "su17cevqhb",
   "metadata": {},
   "source": "## Step 5: Test the Trained Model\n\nAfter training, let's see what the network learned! We'll make predictions on a few examples."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ptbc1of9sx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first 5 samples\n",
    "print(\"Testing trained model on first 5 samples:\\n\")\n",
    "\n",
    "for i in range(min(5, n_samples)):\n",
    "    # Get input and convert to Values\n",
    "    x_test = list(map(Value, X_norm[i]))\n",
    "\n",
    "    # Forward pass\n",
    "    pred = n(x_test)\n",
    "\n",
    "    # Find predicted digit (argmax)\n",
    "    predicted_digit = max(range(10), key=lambda j: pred[j].data)\n",
    "    true_digit = y_small[i]\n",
    "\n",
    "    # Show prediction vs truth\n",
    "    status = \"✓\" if predicted_digit == true_digit else \"✗\"\n",
    "    print(f\"Sample {i}: True={true_digit}, Predicted={predicted_digit} {status}\")\n",
    "\n",
    "    # Show the scores for all 10 digits\n",
    "    print(f\"  Scores: {[f'{pred[j].data:.2f}' for j in range(10)]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
