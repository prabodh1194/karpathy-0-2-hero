{
 "cells": [
  {
   "cell_type": "code",
   "id": "ec9690849226b731",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from micrograd import Value\n",
    "from nn import MLP"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bcf77229ee949c9a",
   "metadata": {},
   "source": [
    "# make up a dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y * 2 - 1  # make y be -1 or 1\n",
    "\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"jet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f24dc1fce71dbce1",
   "metadata": {},
   "source": [
    "# init the MLP model with 2 layers\n",
    "\n",
    "model = MLP(2, [16, 16, 1])\n",
    "print(model, \"number of params\", len(model.parameters()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "842f06d6b004527d",
   "metadata": {},
   "source": [
    "# loss function\n",
    "def loss():\n",
    "    Xb, yb = X, y\n",
    "\n",
    "    # convert input to neuron input value\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "\n",
    "    # forward-pass\n",
    "    scores = [model(x) for x in inputs]\n",
    "\n",
    "    # if the expected & actual score point in the same direction, then\n",
    "    # score & label (yi) will be positive. Further, subtracting from 1\n",
    "    # will make the value < 1 RelU will further trim the values to 0.\n",
    "    # This way the loss will remain low.\n",
    "    # However, if the score & label are in opposite directions, then\n",
    "    # the loss will compulsorily remain above 1.\n",
    "    # Hence in case of correct answer, loss remains in [0, 1] otherwise [1, +inf]\n",
    "\n",
    "    # Hinge loss says: Am I on the right side AND far enough away from the boundary?\n",
    "    # It's like parking - you don't just barely avoid hitting the car next to you,\n",
    "    # you leave some space for safety.\n",
    "\n",
    "    # In the real world, predictions near the boundary are uncertain and risky. A point\n",
    "    # that's barely on the correct side might flip to the wrong side with slight noise\n",
    "    # or new data. Hinge loss says \"I don't trust you until you're confidently correct.\"\n",
    "    # This creates a buffer zone around the decision boundary.\n",
    "\n",
    "    # in this case, the margin is 1. that means, every point is at least 1 unit away from\n",
    "    # the decision boundary.\n",
    "    losses = [(1 + (-yi * scorei)).relu() for scorei, yi in zip(scores, yb)]\n",
    "\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "\n",
    "    # L2 regularization (weight decay): penalizes large weights by adding α * Σ(wi²)\n",
    "    # This prevents overfitting by encouraging smaller, more distributed weights\n",
    "    # rather than relying heavily on any single weight\n",
    "    \"\"\"\n",
    "    This one has multiple beautiful intuitions:\n",
    "\n",
    "Intuition 1 - Occam's Razor:\n",
    "Between two models that fit your data equally well, prefer the simpler one.\n",
    "Small weights = simpler model. A model with weights [100, -80, 95] is \"doing more\"\n",
    "than one with [0.5, -0.3, 0.4], even if they both fit the training data.\n",
    "The simpler one is more likely to generalize.\n",
    "\n",
    "Intuition 2 - Don't put all eggs in one basket:\n",
    "Without regularization, the model might put huge weight on one feature and ignore others.\n",
    "With L2, it's \"cheaper\" (in terms of loss) to use many small weights than one huge weight.\n",
    "Example: To get an output of 10, you could use:\n",
    "\n",
    "One weight: w₁=10 → penalty = 10² = 100\n",
    "Two weights: w₁=5, w₂=5 → penalty = 5² + 5² = 50 (better!)\n",
    "\n",
    "This spreads the \"responsibility\" across features, making the model more robust.\n",
    "\n",
    "Intuition 3 - Noise resilience:\n",
    "Large weights amplify noise. If a feature has a tiny bit of noise and it has\n",
    "weight 1000, that noise gets magnified 1000×. Small weights dampen noise's effect.\n",
    "\n",
    "Intuition 4 - Geometric view:\n",
    "L2 regularization keeps weights in a ball around the origin. During training,\n",
    "you're trying to minimize loss (pulls weights in one direction) while regularization\n",
    "pulls them back toward zero. The final solution is a compromise - good fit without\n",
    "going crazy with weight values.\n",
    "    \"\"\"\n",
    "    alpha = 1e-4\n",
    "    reg_loss = sum((p * p for p in model.parameters())) * alpha\n",
    "\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    # this is more for info purpose.\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for scorei, yi in zip(scores, yb)]\n",
    "\n",
    "    return total_loss, sum(accuracy) / len(accuracy)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40fd75cadf9464ac",
   "metadata": {},
   "source": [
    "Another visual of the margin. Notice how H2 & H3, both are valid classification boundaries, but we can clearly see that H3 is better. This is what hinge-loss is helping us determine.\n",
    "\n",
    "![image.png](attachment:4dfe1a42-ef88-4cac-a2ef-0842e8090c42.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e5847e16ae934e8",
   "metadata": {},
   "source": [
    "# optimisation\n",
    "for k in range(100):\n",
    "    total_loss, acc = loss()\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    learning_rate = 1.0 - 0.9 * k / 100\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    print(f\"step {k} loss {total_loss.data}, accuracy {acc * 100}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b527ee7254fe869",
   "metadata": {},
   "source": [
    "# visualize decision boundary\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdBu)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
