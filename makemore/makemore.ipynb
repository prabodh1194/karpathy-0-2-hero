{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47e463-33ec-4f86-a528-7fa423df79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e5fb2-b759-4548-a45b-3ecd3bcf3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc04fc-9b30-4355-ac9a-88ca36c068b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47441d-c8b4-48a2-8c19-c8036fb5ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9928fa0-8596-4518-8d44-6582fc666f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971e86e-6890-4c60-b85d-be2bb387dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaf65c-b65b-4e28-a193-bdd8255c527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model\n",
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = [\"<S>\"] + list(w) + [\"<E>\"]  # start & end markers on a word\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c07f81-4012-47fa-b07a-345e85170abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b9e54-bbf8-4dc3-aadd-fbb7f3d2b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a05526-e36f-4d92-a594-1bbf5039790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93c0e8-9192-43e0-9e1f-74ff73a8142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628c368-9a98-4de0-accb-995f3a3f9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]  # start & end markers on a word\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242b096-8d80-45db-b826-b755c92b4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# now the first row is just starting word & first column is ending words & everything else are middle characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e451f2-dfcb-49bb-ad14-f2e59afb9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec0747-34ca-4c0a-93a3-a8af988477e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca65e69-b90d-46b5-9a7a-6fa0bb15bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdim=True)  # sum row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002bef3-d54e-4d54-bc9c-21fdba600195",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        # p = N[ix].float()\n",
    "        # p = p / p.sum()\n",
    "        # p = torch.ones(27) / 27.0  # every output is as likely; bigram is better than this randomness!\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993878c9-9dcd-41cf-8aaf-7b7e9cac27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Goal: maximise the likelihood of the data w.r.t. model parameters (statistical modelling) - take product of probablities\n",
    "a * b * c\n",
    "\n",
    "equivalent to maximising the log likelihood (because log is monotonic) - take sum of probabilities\n",
    "log(a * b * c) -> log(a) + log(b) + log(c)\n",
    "\n",
    "equivalent to minimising the negative log likelihood\n",
    "equivalent to minimising the average log likelihood\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1830a3-9512-4059-b78b-a34777050bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking to evaluate the quality of this bigram model now\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    # for w in [\"andrejq\"]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]  # start & end markers on a word\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "# negative log_likelihood\n",
    "nil = -log_likelihood\n",
    "print(f\"{nil=}\")\n",
    "print(f\"{nil / n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c16be-53fb-4405-8a31-e4f5a3cbd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the nn problem:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83751b5-1f48-4adf-ac95-7362600ca2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set for bigram: x, y -> given x predict y\n",
    "# x: inputs\n",
    "# y: targets\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\"\"\"\n",
    "xs & ys are integer indices for characters. to make a neural net aware of this\n",
    "spatial arrangement of data, we can use one-hot encoding. otherwise a NN is not\n",
    "capable of understanding an integer just randomly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c36b28-a4c6-473c-a6ed-c64f0ce8e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b942afd-4e74-418f-86c3-2bf980f2fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbfe50-50be-48cb-8848-431f7c3d63c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()  # nns like floats not ints\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26205795-341e-41c3-b314-2f95c128c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For input word, say, \"emma.\" (5 chars), we make 5 independent forward passes through the network. one per character.\n",
    "\n",
    "- OHE triggers one of the 27 neurons at a time. so 'e' triggs the 5 of the 27 neurons (0-indexed).\n",
    "- The weight multiplication of 27 weights in that neuron is just how 'e' is interatacting with all the possible 27 outputs.\n",
    "- So essentially, matmul acts as a switch. Given a char input, it select all the 27 weights of that neuron as-is.\n",
    "\"\"\"\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()  # nns like floats not ints\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "W = torch.randn(\n",
    "    (27, 27), generator=g\n",
    ")  # first layer of the net; where there are 27 neurons\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afeed1-a803-4ace-b9e5-3ce230dd44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LOGITS — Quick Reference\n",
    "\n",
    "What are they?\n",
    "Raw, unnormalized scores output by a neural network before converting to probabilities.\n",
    "\n",
    "The flow:\n",
    "input → network → LOGITS → softmax → probabilities\n",
    "                  (any real number)   (0 to 1, sum=1)\n",
    "\n",
    "Why \"logits\"?\n",
    "From logistic regression — \"logit\" = log-odds. Just ML jargon for \"pre-softmax values.\"\n",
    "\n",
    "Key properties:\n",
    "- Can be any real number (negative, positive, large, small)\n",
    "- Not probabilities — don't sum to 1\n",
    "- Only relative differences matter (adding a constant to all logits doesn't change final probabilities)\n",
    "\n",
    "Why use them?\n",
    "- Unconstrained optimization — model can freely push scores up/down during training\n",
    "- Softmax handles normalization — separation of concerns\n",
    "- Cleaner gradients — no boundary issues near 0 or 1\n",
    "\n",
    "Example:\n",
    "logits = [0.1, -0.3, 0.9, 0.4, -0.2]   <- raw network output\n",
    "probs  = softmax(logits)               <- [0.19, 0.13, 0.43, 0.26, 0.14]\n",
    "\n",
    "\n",
    "LOG-ODDS — Quick Reference\n",
    "\n",
    "Odds\n",
    "----\n",
    "Another way to express probability.\n",
    "\n",
    "odds = probability of event / probability of NOT event\n",
    "     = p / (1 - p)\n",
    "\n",
    "Example: 70% chance of rain\n",
    "odds = 0.70 / 0.30 = 2.33\n",
    "Meaning: rain is 2.33 times more likely than no rain (\"2.33 to 1 odds\")\n",
    "\n",
    "\n",
    "Log-odds (the \"logit\")\n",
    "----------------------\n",
    "Just take the logarithm of the odds:\n",
    "\n",
    "log-odds = log(p / (1 - p))\n",
    "\n",
    "Why bother?\n",
    "- Probability is bounded: 0 to 1\n",
    "- Odds is bounded: 0 to infinity\n",
    "- Log-odds is unbounded: -infinity to +infinity\n",
    "\n",
    "That's the magic. Log-odds can be any real number, just like neural network logits.\n",
    "\n",
    "\n",
    "The connection\n",
    "--------------\n",
    "p = 0.50  ->  odds = 1.0   ->  log-odds = 0\n",
    "p = 0.70  ->  odds = 2.33  ->  log-odds = +0.85\n",
    "p = 0.30  ->  odds = 0.43  ->  log-odds = -0.85\n",
    "p = 0.99  ->  odds = 99    ->  log-odds = +4.6\n",
    "p = 0.01  ->  odds = 0.01  ->  log-odds = -4.6\n",
    "\n",
    "Notice:\n",
    "- 50/50 sits at zero\n",
    "- More likely = positive\n",
    "- Less likely = negative\n",
    "- Symmetric and unbounded\n",
    "\n",
    "That's why neural network outputs are called \"logits\" — they live in this same unconstrained space.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1541ce-04d0-42ef-b7ad-4dc95c2f5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W  # log-counts\n",
    "counts = logits.exp()  # equivalent to the N-matrix that we had created earlier\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "# last 2 lines are softmax: for a given linear output convert to probabilities\n",
    "# this is the progression. we are\n",
    "# probability → odds → log-odds (logit)\n",
    "# [0 to 1]      [0 to ∞]   [-∞ to +∞]\n",
    "\"\"\"\n",
    "LOGITS TO PROBABILITIES — The Full Pipeline\n",
    "\n",
    "The trifecta\n",
    "------------\n",
    "probability -> odds -> log-odds (logit)\n",
    "[0 to 1]      [0 to inf]   [-inf to +inf]\n",
    "\n",
    "Each step \"unlocks\" more of the number line.\n",
    "They're all just different ways of saying the same thing — fully convertible.\n",
    "\n",
    "\n",
    "Going backwards\n",
    "---------------\n",
    "logits -> exp() -> odds -> normalize -> probabilities\n",
    "\n",
    "Exponentiate log-odds to get odds.\n",
    "Normalize odds to get probabilities.\n",
    "\n",
    "\n",
    "Softmax does both steps in one\n",
    "------------------------------\n",
    "softmax(logits) = exp(logits) / sum(exp(logits))\n",
    "                      |              |\n",
    "                 get odds      normalize them\n",
    "\n",
    "So when you call softmax, you're really doing:\n",
    "1. Exponentiate to get back to odds-space\n",
    "2. Divide by the total so they sum to 1\n",
    "\n",
    "That's the whole trick. Softmax = \"exp then normalize.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a020e23-9e93-46e9-a847-a15ec73d94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print(f\"bigram example {i + 1}: {itos[x]}{itos[y]} (indexes {x}, {y})\")\n",
    "    print(\"input to the neural net:\", x)\n",
    "    print(\"output probabilities from the neural net:\", probs[i])\n",
    "    print(\"label (actual next character):\", y)\n",
    "    p = probs[i, y]\n",
    "    print(\"probability assigned by the net to the correct character:\", p.item())\n",
    "    logp = torch.log(p)\n",
    "    print(\"log likelihood:\", logp.item())\n",
    "    nll = -logp\n",
    "    print(\"negative log likelihood:\", nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print(\"==============\")\n",
    "print(\"avg negative log likelihood\", nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0ee94-0695-4b7e-976f-cfdfaa789ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae8250-58e9-43f7-be48-58d0b2a9293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(\n",
    "    (27, 27), generator=g, requires_grad=True\n",
    ")  # first layer of the net; where there are 27 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9f2f6-4968-4790-ac87-15f1ffa093ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()  # nns like floats not ints\n",
    "logits = xenc @ W  # log-counts\n",
    "counts = logits.exp()  # equivalent to the N-matrix that we had created earlier\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f9b1f73769a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "there is also some math on regularisation\n",
    "\n",
    "REGULARIZATION — The Math\n",
    "\n",
    "The problem\n",
    "-----------\n",
    "Model overfits = memorizes training data, fails on new data.\n",
    "Solution: penalize complexity (big weights).\n",
    "\n",
    "Without regularization\n",
    "----------------------\n",
    "loss = prediction_loss\n",
    "\n",
    "With regularization (L2 / weight decay)\n",
    "---------------------------------------\n",
    "loss = prediction_loss + lambda * sum(W^2)\n",
    "\n",
    "Where:\n",
    "- W = all your weights\n",
    "- W^2 = square each weight (makes all positive, penalizes big ones more)\n",
    "- sum(W^2) = add them all up\n",
    "- lambda = how much you care about keeping weights small (you tune this)\n",
    "\n",
    "Concrete example\n",
    "----------------\n",
    "W = [0.5, -2.0, 0.1, 3.0]\n",
    "W^2 = [0.25, 4.0, 0.01, 9.0]\n",
    "sum(W^2) = 13.26\n",
    "\n",
    "If lambda = 0.01:\n",
    "regularization penalty = 0.01 * 13.26 = 0.1326\n",
    "\n",
    "If prediction loss = 2.5:\n",
    "total loss = 2.5 + 0.1326 = 2.6326\n",
    "\n",
    "The gradient (backprop)\n",
    "-----------------------\n",
    "For any weight w, the regularization term w^2 contributes:\n",
    "\n",
    "d/dw (lambda * w^2) = 2 * lambda * w\n",
    "\n",
    "So gradient has two parts:\n",
    "gradient = gradient_from_prediction + 2 * lambda * w\n",
    "\n",
    "Big weights get pushed harder toward zero (push is proportional to w).\n",
    "\n",
    "Why \"weight decay\"?\n",
    "-------------------\n",
    "Weights decay toward zero a little bit each step:\n",
    "\n",
    "w_new = w_old - learning_rate * pred_gradient - learning_rate * 2 * lambda * w_old\n",
    "                    ^                       ^\n",
    "             fit the data            decay toward zero\n",
    "\n",
    "Same math, different name.\n",
    "\n",
    "The effect\n",
    "----------\n",
    "- Without: weights can grow large, model overfits\n",
    "- With: weights stay small unless truly needed, model generalizes better\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa4a96-8582-44ca-8c46-70d91e2ac69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aac473-32ec-4f26-b3c1-58d6800c2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e7821-39f8-4571-ac95-2dcaac36b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b24f5-72bc-41df-bdec-567a100a3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b130a-6f83-4968-9b86-2ee796fbf775",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "\n",
    "print(\"number of examples\", num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(\n",
    "    (27, 27), generator=g, requires_grad=True\n",
    ")  # first layer of the net; where there are 27 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5bf61-0397-4d96-8ba9-7dd657f5e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    # forward pass\n",
    "\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()  # nns like floats not ints\n",
    "    logits = xenc @ W  # log-counts\n",
    "    counts = logits.exp()  # equivalent to the N-matrix that we had created earlier\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b89d71cb-ffa4-42c7-8782-58890215b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# sample the nn\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # before\n",
    "        # p = P[ix]\n",
    "        # ------\n",
    "\n",
    "        # now\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W  # predicts log-counts\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        # ------\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(out))\n",
    "\n",
    "# ps: output remains very very similar cuz this bigram NN is pretty much the same as what we trained earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
